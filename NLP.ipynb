{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f76aab93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/diya/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/diya/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import unicodedata\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def rem_en(input_txt):\n",
    "    words = input_txt.lower().split()\n",
    "    noise_free_words = [word for word in words if word not in stop] \n",
    "    noise_free_text = \" \".join(noise_free_words)\n",
    "    return noise_free_text\n",
    "\n",
    "stop = set(stopwords.words('english'))\n",
    "tokeniser = RegexpTokenizer(r\"\\w+\")\n",
    "lemmatiser = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "321ec2c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2393392 entries, 0 to 2393391\n",
      "Data columns (total 14 columns):\n",
      " #   Column             Dtype  \n",
      "---  ------             -----  \n",
      " 0   Unnamed: 0         int64  \n",
      " 1   customer_id        int64  \n",
      " 2   review_id          object \n",
      " 3   product_id         object \n",
      " 4   product_parent     int64  \n",
      " 5   product_title      object \n",
      " 6   star_rating        float64\n",
      " 7   helpful_votes      float64\n",
      " 8   total_votes        float64\n",
      " 9   vine               object \n",
      " 10  verified_purchase  object \n",
      " 11  review_headline    object \n",
      " 12  review_body        object \n",
      " 13  review_date        object \n",
      "dtypes: float64(3), int64(3), object(8)\n",
      "memory usage: 255.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('preprocessed_amazon_customer_reviews.csv')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3b750b4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>review_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_parent</th>\n",
       "      <th>product_title</th>\n",
       "      <th>star_rating</th>\n",
       "      <th>helpful_votes</th>\n",
       "      <th>total_votes</th>\n",
       "      <th>vine</th>\n",
       "      <th>verified_purchase</th>\n",
       "      <th>review_headline</th>\n",
       "      <th>review_body</th>\n",
       "      <th>review_date</th>\n",
       "      <th>review_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>42521656</td>\n",
       "      <td>R26MV8D0KG6QI6</td>\n",
       "      <td>B000SAQCWC</td>\n",
       "      <td>159713740</td>\n",
       "      <td>The Cravings Place Chocolate Chunk Cookie Mix,...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Using these for years - love them.</td>\n",
       "      <td>As a family allergic to wheat, dairy, eggs, nu...</td>\n",
       "      <td>2015-08-31</td>\n",
       "      <td>Using these for years - love them. . As a fami...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>12049833</td>\n",
       "      <td>R1OF8GP57AQ1A0</td>\n",
       "      <td>B00509LVIQ</td>\n",
       "      <td>138680402</td>\n",
       "      <td>Mauna Loa Macadamias, 11 Ounce Packages</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Wonderful</td>\n",
       "      <td>My favorite nut.  Creamy, crunchy, salty, and ...</td>\n",
       "      <td>2015-08-31</td>\n",
       "      <td>Wonderful . My favorite nut.  Creamy, crunchy,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>107642</td>\n",
       "      <td>R3VDC1QB6MC4ZZ</td>\n",
       "      <td>B00KHXESLC</td>\n",
       "      <td>252021703</td>\n",
       "      <td>Organic Matcha Green Tea Powder - 100% Pure Ma...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>This green tea tastes so good! My girlfriend l...</td>\n",
       "      <td>2015-08-31</td>\n",
       "      <td>Five Stars . This green tea tastes so good! My...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>6042304</td>\n",
       "      <td>R12FA3DCF8F9ER</td>\n",
       "      <td>B000F8JIIC</td>\n",
       "      <td>752728342</td>\n",
       "      <td>15oz Raspberry Lyons Designer Dessert Syrup Sauce</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>I love Melissa's brand but this is a great sec...</td>\n",
       "      <td>2015-08-31</td>\n",
       "      <td>Five Stars . I love Melissa's brand but this i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>18123821</td>\n",
       "      <td>RTWHVNV6X4CNJ</td>\n",
       "      <td>B004ZWR9RQ</td>\n",
       "      <td>552138758</td>\n",
       "      <td>Stride Spark Kinetic Fruit Sugar Free Gum, 14-...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>good</td>\n",
       "      <td>2015-08-31</td>\n",
       "      <td>Five Stars . good</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  customer_id       review_id  product_id  product_parent  \\\n",
       "0           0     42521656  R26MV8D0KG6QI6  B000SAQCWC       159713740   \n",
       "1           1     12049833  R1OF8GP57AQ1A0  B00509LVIQ       138680402   \n",
       "2           2       107642  R3VDC1QB6MC4ZZ  B00KHXESLC       252021703   \n",
       "3           3      6042304  R12FA3DCF8F9ER  B000F8JIIC       752728342   \n",
       "4           4     18123821   RTWHVNV6X4CNJ  B004ZWR9RQ       552138758   \n",
       "\n",
       "                                       product_title  star_rating  \\\n",
       "0  The Cravings Place Chocolate Chunk Cookie Mix,...          5.0   \n",
       "1            Mauna Loa Macadamias, 11 Ounce Packages          5.0   \n",
       "2  Organic Matcha Green Tea Powder - 100% Pure Ma...          5.0   \n",
       "3  15oz Raspberry Lyons Designer Dessert Syrup Sauce          5.0   \n",
       "4  Stride Spark Kinetic Fruit Sugar Free Gum, 14-...          5.0   \n",
       "\n",
       "   helpful_votes  total_votes vine verified_purchase  \\\n",
       "0            0.0          0.0    N                 Y   \n",
       "1            0.0          0.0    N                 Y   \n",
       "2            0.0          0.0    N                 N   \n",
       "3            0.0          0.0    N                 Y   \n",
       "4            0.0          0.0    N                 Y   \n",
       "\n",
       "                      review_headline  \\\n",
       "0  Using these for years - love them.   \n",
       "1                           Wonderful   \n",
       "2                          Five Stars   \n",
       "3                          Five Stars   \n",
       "4                          Five Stars   \n",
       "\n",
       "                                         review_body review_date  \\\n",
       "0  As a family allergic to wheat, dairy, eggs, nu...  2015-08-31   \n",
       "1  My favorite nut.  Creamy, crunchy, salty, and ...  2015-08-31   \n",
       "2  This green tea tastes so good! My girlfriend l...  2015-08-31   \n",
       "3  I love Melissa's brand but this is a great sec...  2015-08-31   \n",
       "4                                               good  2015-08-31   \n",
       "\n",
       "                                         review_text  \n",
       "0  Using these for years - love them. . As a fami...  \n",
       "1  Wonderful . My favorite nut.  Creamy, crunchy,...  \n",
       "2  Five Stars . This green tea tastes so good! My...  \n",
       "3  Five Stars . I love Melissa's brand but this i...  \n",
       "4                                  Five Stars . good  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#deal with NaN values in review heading / body columns \n",
    "df[['review_headline', 'review_body']] = df[['review_headline', 'review_body']].fillna('')\n",
    "\n",
    "# merge review text columns \n",
    "df['review_text'] = df.review_headline.str.cat(df.review_body, sep=' . ')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9770ecce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_extra_whitespaces_func(text):\n",
    "    return re.sub(r'^\\s*|\\s\\s*', ' ', text).strip()\n",
    "\n",
    "def remove_accented_chars_func(text):\n",
    "    return unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "\n",
    "def remove_html_tags_func(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    # Get all the text other than html tags.\n",
    "    stripped_text = soup.get_text(separator=\" \")\n",
    "    return stripped_text\n",
    "\n",
    "def deEmojify(inputString):\n",
    "    return inputString.encode('ascii', 'ignore').decode('ascii')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6e6130ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove html tags\n",
    "df[\"clean_text\"] = df[\"review_text\"].apply(remove_html_tags_func)\n",
    "\n",
    "# remove links\n",
    "df[\"clean_text\"] = df[\"clean_text\"].apply(lambda s: ' '.join(re.sub(r'https?://\\S+|www\\.\\S+', \"\", s).split()))\n",
    "\n",
    "## remove punctuation\n",
    "df[\"clean_text\"] = df[\"clean_text\"].apply(lambda s: ' '.join(re.sub(\"[.,!?:;-='...@#_]\", \"\", s).split()))\n",
    "\n",
    "# remove numbers\n",
    "df[\"clean_text\"].replace('\\d+', '', regex=True, inplace=True)\n",
    "\n",
    "#remove emojis\n",
    "df[\"clean_text\"] = df[\"clean_text\"].apply(lambda s: deEmojify(s))\n",
    "\n",
    "#remove accented words\n",
    "df[\"clean_text\"] = df[\"clean_text\"].apply(remove_accented_chars_func)\n",
    "\n",
    "#remove extra white spaces \n",
    "df[\"clean_text\"] = df[\"clean_text\"].apply(remove_extra_whitespaces_func)\n",
    "\n",
    "#make all words lowercase\n",
    "df[\"clean_text\"] = df[\"clean_text\"].apply(lambda s: s.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "62166934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          review_text  \\\n",
      "0   Using these for years - love them. . As a fami...   \n",
      "1   Wonderful . My favorite nut.  Creamy, crunchy,...   \n",
      "2   Five Stars . This green tea tastes so good! My...   \n",
      "3   Five Stars . I love Melissa's brand but this i...   \n",
      "4                                   Five Stars . good   \n",
      "5                  Not Happy . The popcorn was stale.   \n",
      "6   Five Stars . Love these bars, but have to watc...   \n",
      "7   Five Stars . Love the taste but the price was ...   \n",
      "8   Great tasting! . I'm a member of the crowdtap ...   \n",
      "9   Disgusting now and difficult on digestion . Us...   \n",
      "10  If you like soy sauce, you'll really like this...   \n",
      "11  Four Stars . Good flavor and seems concentrate...   \n",
      "12              Five Stars . Great to use in recipes.   \n",
      "13  Five Stars . It's rice. Have enough to last fo...   \n",
      "14  Five Stars . Very good tasting and a great way...   \n",
      "15  Excellent . They were perfect and came right o...   \n",
      "16  YUMMY! . Wow, these are sooooooo good and I am...   \n",
      "17  1 Out Of 5 Of My Co-Workers Thought It Was \"Ok...   \n",
      "18  delicious tea and easy to make . I love this t...   \n",
      "19  Best coconut oil . I have used servo different...   \n",
      "\n",
      "                                           clean_text  \n",
      "0   using these for years - love them as a family ...  \n",
      "1   wonderful my favorite nut creamy crunchy salty...  \n",
      "2   five stars this green tea tastes so good my gi...  \n",
      "3   five stars i love melissas brand but this is a...  \n",
      "4                                     five stars good  \n",
      "5                     not happy the popcorn was stale  \n",
      "6   five stars love these bars but have to watch o...  \n",
      "7   five stars love the taste but the price was to...  \n",
      "8   great tasting im a member of the crowdtap jif ...  \n",
      "9   disgusting now and difficult on digestion used...  \n",
      "10  if you like soy sauce youll really like this -...  \n",
      "11  four stars good flavor and seems concentrate e...  \n",
      "12                 five stars great to use in recipes  \n",
      "13  five stars its rice have enough to last for mo...  \n",
      "14  five stars very good tasting and a great way t...  \n",
      "15  excellent they were perfect and came right on ...  \n",
      "16  yummy wow these are sooooooo good and i am not...  \n",
      "17  out of of my co-workers thought it was \"okay\" ...  \n",
      "18  delicious tea and easy to make i love this tea...  \n",
      "19  best coconut oil i have used servo different b...  \n"
     ]
    }
   ],
   "source": [
    "#checking processed text\n",
    "df_processed = df[['review_text', 'clean_text']].head(20)\n",
    "print(df_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1262929c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from contractions import CONTRACTION_MAP \n",
    "\n",
    "def expand_contractions(text, map=CONTRACTION_MAP):\n",
    "    pattern = re.compile('({})'.format('|'.join(map.keys())), flags=re.IGNORECASE|re.DOTALL)\n",
    "    def get_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        expanded = map.get(match) if map.get(match) else map.get(match.lower())\n",
    "        expanded = first_char+expanded[1:]\n",
    "        return expanded     \n",
    "    new_text = pattern.sub(get_match, text)\n",
    "    new_text = re.sub(\"'\", \"\", new_text)\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07fe6e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove stop words\n",
    "df[\"clean_text\"] = df[\"clean_text\"].apply(lambda s: rem_en(s))\n",
    "\n",
    "#tokenise words\n",
    "df[\"clean_text\"] = df[\"clean_text\"].apply(lambda x: tokeniser.tokenize(x))\n",
    "\n",
    "#lemmatise words\n",
    "df[\"clean_text\"] = df[\"clean_text\"].apply(lambda tokens: [lemmatiser.lemmatize(token, pos='v') for token in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5028735a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['clean_text'].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3dff729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# POS tagging\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk import pos_tag\n",
    "\n",
    "df['tagged'] = df['clean_text'].apply(nltk.pos_tag)\n",
    "\n",
    "#extract nouns\n",
    "df['nouns'] = df['tagged'].apply(lambda x: [word for word, tag in x if tag in ['NN', 'NNS', 'NNP', 'NNPS']])\n",
    "# extract verbs\n",
    "df['verbs'] = df['tagged'].apply(lambda x: [word for word, tag in x if tag in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']])\n",
    "\n",
    "df_pos_tagged = df[['clean_text', 'nouns', 'verbs', 'tagged']].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e45d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunking \n",
    "\n",
    "#Define the patterns we had identified\n",
    "#Chunk 1 : Adjective followed by a Noun \n",
    "chunkGram = r\"\"\"chunk: {<JJ>+<NN>+}\"\"\"\n",
    "\n",
    "#Chunk 2 : Noun and Adjective with other POS in between\n",
    "chunkGram = r\"\"\"chunk: {<NN|NNP|NNS|NNPS>+<IN|DT|NN|VB.|RB>*<JJ>+}\"\"\"\n",
    "\n",
    "#Chunk 3: Sequence of Nouns\n",
    "chunkGram = r\"\"\"chunk: {<NN|NNP|NNS|NNPS>{2,9}}\"\"\"\n",
    "\n",
    "\n",
    "#Passing the Chunk to a regex parser\n",
    "chunkParser = nltk.RegexpParser(chunkGram)\n",
    "#Parsing\n",
    "chunked = chunkParser.parse(tagged)\n",
    "print(chunked)\n",
    "\n",
    "\n",
    "# Accessing the Chunk\n",
    "for subtree in chunked.subtrees(filter=lambda t: t.label() == 'chunk'):\n",
    "    print('Filtered chunks= ',subtree)\n",
    "    chunked_output = ' '.join([w for w, t in subtree.leaves()])\n",
    "     #Visualize the output\n",
    "     chunked.draw()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "cc9e64ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          review_text  \\\n",
      "0   Using these for years - love them. . As a fami...   \n",
      "1   Wonderful . My favorite nut.  Creamy, crunchy,...   \n",
      "2   Five Stars . This green tea tastes so good! My...   \n",
      "3   Five Stars . I love Melissa's brand but this i...   \n",
      "4                                   Five Stars . good   \n",
      "5                  Not Happy . The popcorn was stale.   \n",
      "6   Five Stars . Love these bars, but have to watc...   \n",
      "7   Five Stars . Love the taste but the price was ...   \n",
      "8   Great tasting! . I'm a member of the crowdtap ...   \n",
      "9   Disgusting now and difficult on digestion . Us...   \n",
      "10  If you like soy sauce, you'll really like this...   \n",
      "11  Four Stars . Good flavor and seems concentrate...   \n",
      "12              Five Stars . Great to use in recipes.   \n",
      "13  Five Stars . It's rice. Have enough to last fo...   \n",
      "14  Five Stars . Very good tasting and a great way...   \n",
      "15  Excellent . They were perfect and came right o...   \n",
      "16  YUMMY! . Wow, these are sooooooo good and I am...   \n",
      "17  1 Out Of 5 Of My Co-Workers Thought It Was \"Ok...   \n",
      "18  delicious tea and easy to make . I love this t...   \n",
      "19  Best coconut oil . I have used servo different...   \n",
      "\n",
      "                                           clean_text  \\\n",
      "0   use years love family allergic wheat dairy egg...   \n",
      "1   wonderful favorite nut creamy crunchy salty sl...   \n",
      "2      five star green tea taste good girlfriend love   \n",
      "3   five star love melissas brand great second can...   \n",
      "4                                      five star good   \n",
      "5                                 happy popcorn stale   \n",
      "6            five star love bar watch fat sugar grams   \n",
      "7    five star love taste price high doesnt come pack   \n",
      "8   great taste im member crowdtap jif program jif...   \n",
      "9   disgust difficult digestion use decent product...   \n",
      "10  like soy sauce youll really like cannot tell d...   \n",
      "11  four star good flavor seem concentrate enough ...   \n",
      "12                        five star great use recipes   \n",
      "13                  five star rice enough last months   \n",
      "14      five star good taste great way get away sugar   \n",
      "15                  excellent perfect come right time   \n",
      "16  yummy wow sooooooo good sweet tooth full nut a...   \n",
      "17  co workers think okay buy local super market w...   \n",
      "18  delicious tea easy make love tea delicious sin...   \n",
      "19  best coconut oil use servo different brand coc...   \n",
      "\n",
      "                                           final_text  \n",
      "0   use years love family allergic wheat dairy egg...  \n",
      "1   wonderful favorite nut creamy crunchy salty sl...  \n",
      "2      five star green tea taste good girlfriend love  \n",
      "3   five star love melissas brand great second can...  \n",
      "4                                      five star good  \n",
      "5                                 happy popcorn stale  \n",
      "6            five star love bar watch fat sugar grams  \n",
      "7    five star love taste price high doesnt come pack  \n",
      "8   great taste im member crowdtap jif program jif...  \n",
      "9   disgust difficult digestion use decent product...  \n",
      "10  like soy sauce youll really like cannot tell d...  \n",
      "11  four star good flavor seem concentrate enough ...  \n",
      "12                        five star great use recipes  \n",
      "13                  five star rice enough last months  \n",
      "14      five star good taste great way get away sugar  \n",
      "15                  excellent perfect come right time  \n",
      "16  yummy wow sooooooo good sweet tooth full nut a...  \n",
      "17  co workers think okay buy local super market w...  \n",
      "18  delicious tea easy make love tea delicious sin...  \n",
      "19  best coconut oil use servo different brand coc...  \n"
     ]
    }
   ],
   "source": [
    "#create sample dataframe - first 20 reviews\n",
    "test_df = df[['review_text', 'clean_text']].head(20)\n",
    "final_text = []\n",
    "for i in range(len(test_df)):\n",
    "    tokenised_list = df.loc[i, \"clean_text\"]\n",
    "    tokenised_text = ''.join(tokenised_list)\n",
    "    final_text.append(tokenised_text)\n",
    "test_df[\"final_text\"] = final_text\n",
    "print(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "94864296",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>add</th>\n",
       "      <th>addict</th>\n",
       "      <th>allergens</th>\n",
       "      <th>allergic</th>\n",
       "      <th>allergy</th>\n",
       "      <th>allow</th>\n",
       "      <th>almond</th>\n",
       "      <th>also</th>\n",
       "      <th>amaze</th>\n",
       "      <th>aminos</th>\n",
       "      <th>...</th>\n",
       "      <th>whenever</th>\n",
       "      <th>whim</th>\n",
       "      <th>without</th>\n",
       "      <th>wonderful</th>\n",
       "      <th>workers</th>\n",
       "      <th>would</th>\n",
       "      <th>wow</th>\n",
       "      <th>years</th>\n",
       "      <th>youll</th>\n",
       "      <th>yummy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.271162</td>\n",
       "      <td>0.135581</td>\n",
       "      <td>0.135581</td>\n",
       "      <td>0.135581</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.119178</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.271162</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.337031</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.187001</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.087631</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.220963</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.097115</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.110482</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.348397</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.361542</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.361542</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.361542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.077594</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.077594</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.077594</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.155187</td>\n",
       "      <td>0.155187</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.124728</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.095318</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows × 258 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         add    addict  allergens  allergic   allergy     allow    almond  \\\n",
       "0   0.000000  0.000000   0.271162  0.135581  0.135581  0.135581  0.000000   \n",
       "1   0.000000  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2   0.000000  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "3   0.000000  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4   0.000000  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "5   0.000000  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "6   0.000000  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "7   0.000000  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "8   0.000000  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "9   0.000000  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "10  0.000000  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "11  0.348397  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "12  0.000000  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "13  0.000000  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "14  0.000000  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "15  0.000000  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "16  0.000000  0.361542   0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "17  0.000000  0.000000   0.000000  0.000000  0.000000  0.000000  0.077594   \n",
       "18  0.000000  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "19  0.000000  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "        also     amaze    aminos  ...  whenever      whim   without  \\\n",
       "0   0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.119178   \n",
       "1   0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "2   0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "3   0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "4   0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "5   0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "6   0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "7   0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "8   0.000000  0.000000  0.000000  ...  0.187001  0.000000  0.000000   \n",
       "9   0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "10  0.087631  0.000000  0.220963  ...  0.000000  0.000000  0.097115   \n",
       "11  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "12  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "13  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "14  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "15  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "16  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "17  0.000000  0.077594  0.000000  ...  0.000000  0.077594  0.000000   \n",
       "18  0.124728  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "19  0.095318  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "\n",
       "    wonderful   workers     would       wow     years     youll     yummy  \n",
       "0    0.000000  0.000000  0.000000  0.000000  0.271162  0.000000  0.000000  \n",
       "1    0.337031  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "2    0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "3    0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "4    0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "5    0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "6    0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "7    0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "8    0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "9    0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "10   0.000000  0.000000  0.000000  0.000000  0.000000  0.110482  0.000000  \n",
       "11   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "12   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "13   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "14   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "15   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "16   0.000000  0.000000  0.000000  0.361542  0.000000  0.000000  0.361542  \n",
       "17   0.000000  0.155187  0.155187  0.000000  0.000000  0.000000  0.000000  \n",
       "18   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "19   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "\n",
       "[20 rows x 258 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TF-IDF\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer= TfidfVectorizer(lowercase = False)\n",
    "tfidf_response = vectorizer.fit_transform(test_df['final_text'])\n",
    "df_tfidf_sklearn = pd.DataFrame(tfidf_response.toarray(),columns=vectorizer.get_feature_names())\n",
    "df_tfidf_sklearn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a35da168",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>u</th>\n",
       "      <th>s</th>\n",
       "      <th>e</th>\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>a</th>\n",
       "      <th>r</th>\n",
       "      <th>l</th>\n",
       "      <th>o</th>\n",
       "      <th>v</th>\n",
       "      <th>...</th>\n",
       "      <th>t</th>\n",
       "      <th>d</th>\n",
       "      <th>n</th>\n",
       "      <th>p</th>\n",
       "      <th>b</th>\n",
       "      <th>k</th>\n",
       "      <th>x</th>\n",
       "      <th>j</th>\n",
       "      <th>q</th>\n",
       "      <th>z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>17</td>\n",
       "      <td>39</td>\n",
       "      <td>49</td>\n",
       "      <td>8</td>\n",
       "      <td>22</td>\n",
       "      <td>21</td>\n",
       "      <td>23</td>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>18</td>\n",
       "      <td>3</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    u   s   e      y   a   r   l   o  v  ...   t  d   n  p  b  k  x  j  q  z\n",
       "0  10  17  39  49  8  22  21  23  15  4  ...  18  3  16  2  2  1  2  0  0  0\n",
       "1   0   8   7  10  0   7   4   3   2  2  ...   4  3   4  0  2  0  0  0  0  0\n",
       "2   0   2   7   7  0   3   4   2   3  2  ...   4  2   2  0  0  0  0  0  0  0\n",
       "\n",
       "[3 rows x 27 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#bow \n",
    "def calculateBOW(wordset,l_doc):\n",
    "  tf_diz = dict.fromkeys(wordset,0)\n",
    "  for word in l_doc:\n",
    "      tf_diz[word]=l_doc.count(word)\n",
    "  return tf_diz\n",
    "\n",
    "\n",
    "full_text = []\n",
    "for i in test_df[\"clean_text\"]:\n",
    "    full_text += i\n",
    "\n",
    "wordset = pd.unique(full_text)\n",
    "bow1 = calculateBOW(wordset,test_df[\"clean_text\"][0])\n",
    "bow2 = calculateBOW(wordset,test_df[\"clean_text\"][3])\n",
    "bow3 = calculateBOW(wordset,test_df[\"clean_text\"][2])\n",
    "df_bow = pd.DataFrame([bow1,bow2,bow3])\n",
    "df_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4dfba1e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenized review sample:\n",
      "['use', 'years', 'love', 'family', 'allergic', 'wheat', 'dairy', 'egg', 'nut', 'several', 'things', 'love', 'entire', 'crave', 'place', 'line', 'products', 'allow', 'us', 'bake', 'treat', 'minimal', 'effort', 'ingredients', 'allergy', 'free', 'gluten', 'free', 'mix', 'usually', 'omit', 'one', 'two', 'allergens', 'great', 'see', 'mix', 'create', 'without', 'many', 'common', 'allergens', 'note', 'still', 'soy', 'corn', 'consume', 'regular', 'basis', 'years']\n",
      "==================================================\n",
      "review count of words sample:\n",
      "{'use': 1, 'years': 2, 'love': 2, 'family': 1, 'allergic': 1, 'wheat': 1, 'dairy': 1, 'egg': 1, 'nut': 1, 'several': 1, 'things': 1, 'entire': 1, 'crave': 1, 'place': 1, 'line': 1, 'products': 1, 'allow': 1, 'us': 1, 'bake': 1, 'treat': 1, 'minimal': 1, 'effort': 1, 'ingredients': 1, 'allergy': 1, 'free': 2, 'gluten': 1, 'mix': 2, 'usually': 1, 'omit': 1, 'one': 1, 'two': 1, 'allergens': 2, 'great': 1, 'see': 1, 'create': 1, 'without': 1, 'many': 1, 'common': 1, 'note': 1, 'still': 1, 'soy': 1, 'corn': 1, 'consume': 1, 'regular': 1, 'basis': 1}\n"
     ]
    }
   ],
   "source": [
    "#  Word2Vec data preparation\n",
    "\n",
    "#Create a list of list of words for each sentence or review : Word2Vec requires a list of words for each review.\n",
    "def data_preparation_w2v(dataset):\n",
    "    \"\"\"\n",
    "    param dataset: list of documents.\n",
    "    returns: returns a list of tokenize sentences along with their word count document wise.\n",
    "    \"\"\"\n",
    "    \n",
    "    word_2_vec_list=[]\n",
    "    count_list_document_wise = []\n",
    "    \n",
    "    for review in dataset:\n",
    "        review_list=[]\n",
    "        count_dict = {}\n",
    "        \n",
    "        for word in str(review).split():\n",
    "            review_list.append(word)\n",
    "            if word not in count_dict:\n",
    "                count_dict.update({word: 1})\n",
    "            else:\n",
    "                count_dict.update({word: count_dict[word] + 1})\n",
    "        \n",
    "        word_2_vec_list.append(review_list)\n",
    "        count_list_document_wise.append(count_dict)\n",
    "    \n",
    "    print(f\"tokenized review sample:\\n{word_2_vec_list[0]}\")\n",
    "    print('='*50)\n",
    "    print(f\"review count of words sample:\\n{count_list_document_wise[0]}\")\n",
    "    \n",
    "    return word_2_vec_list, count_list_document_wise\n",
    "\n",
    "\n",
    "tokenized_reviews_LoL, word_count_reviews_LoD = data_preparation_w2v(test_df['final_text'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f935c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WordCloud"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
